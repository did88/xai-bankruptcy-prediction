"""DART 2023-2024ë…„ ì¬ë¬´ì œí‘œ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
ê¸°ì¡´ 2015-2022ë…„ ë°ì´í„°ì™€ ë™ì¼í•œ ì¡°ê±´ìœ¼ë¡œ 2023-2024ë…„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

import asyncio
import os
from pathlib import Path
from typing import List, Tuple
import pandas as pd
from datetime import datetime
import argparse

# ê¸°ì¡´ ëª¨ë“ˆ import
from dart_bulk_downloader import (
    fetch_corp_codes,
    filter_kospi_kosdaq_non_financial,
    fetch_bulk_statements,
    save_to_excel
)


def split_corps_for_teams(corp_codes: List[str], chunk_size: int = 100) -> List[Tuple[int, List[str]]]:
    """ê¸°ì—… ì½”ë“œë¥¼ íŒ€ë³„ë¡œ ë¶„í• """
    chunks = []
    for i in range(0, len(corp_codes), chunk_size):
        team_num = i // chunk_size + 1
        chunk = corp_codes[i:i + chunk_size]
        chunks.append((team_num, chunk))
    return chunks


async def download_team_data_2023_2024(
    api_key: str,
    team_num: int,
    corp_codes: List[str],
    years: range,
    output_dir: Path,
    workers: int = 10
) -> Path:
    """íŠ¹ì • íŒ€ì˜ 2023-2024ë…„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
    print(f"ğŸš€ íŒ€ {team_num} (2023-2024) ë‹¤ìš´ë¡œë“œ ì‹œì‘ - {len(corp_codes)}ê°œ ê¸°ì—…")
    
    start_time = datetime.now()
    
    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    statements = await fetch_bulk_statements(api_key, corp_codes, years, workers)
    
    # ì €ì¥ ê²½ë¡œ ìƒì„± (2023-2024 êµ¬ë¶„ì„ ìœ„í•´ íŒŒì¼ëª… ìˆ˜ì •)
    filename = f"dart_statements_2023_2024_team_{team_num:02d}.xlsx"
    output_path = output_dir / filename
    
    # ì—‘ì…€ë¡œ ì €ì¥
    save_to_excel(statements, output_path)
    
    elapsed = datetime.now() - start_time
    print(f"âœ… íŒ€ {team_num} (2023-2024) ì™„ë£Œ - {elapsed.total_seconds():.1f}ì´ˆ ì†Œìš”")
    print(f"   ì €ì¥ ìœ„ì¹˜: {output_path}")
    print(f"   ë°ì´í„° í–‰ ìˆ˜: {len(statements):,}")
    
    return output_path


def merge_team_files_2023_2024(team_files: List[Path], output_path: Path) -> None:
    """2023-2024ë…„ íŒ€ë³„ íŒŒì¼ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•©"""
    print("\nğŸ“Š 2023-2024ë…„ íŒ€ë³„ íŒŒì¼ ë³‘í•© ì¤‘...")
    
    all_data = []
    for file_path in sorted(team_files):
        if file_path.exists():
            df = pd.read_excel(file_path, engine='openpyxl')
            all_data.append(df)
            print(f"   - {file_path.name}: {len(df):,}í–‰")
    
    if all_data:
        merged_df = pd.concat(all_data, ignore_index=True)
        save_to_excel(merged_df, output_path)
        print(f"\nâœ… 2023-2024ë…„ ë°ì´í„° ë³‘í•© ì™„ë£Œ!")
        print(f"   ì „ì²´ ë°ì´í„°: {len(merged_df):,}í–‰")
        print(f"   ì €ì¥ ìœ„ì¹˜: {output_path}")
        
        # ì—°ë„ë³„ ë°ì´í„° ë¶„í¬ í™•ì¸
        if 'bsns_year' in merged_df.columns:
            year_counts = merged_df['bsns_year'].value_counts().sort_index()
            print(f"   ì—°ë„ë³„ ë¶„í¬:")
            for year, count in year_counts.items():
                print(f"     {year}ë…„: {count:,}ê°œ ê¸°ì—…")
    else:
        print("âŒ ë³‘í•©í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")


def merge_with_existing_data(
    new_data_path: Path, 
    existing_data_path: Path, 
    final_output_path: Path
) -> None:
    """2023-2024ë…„ ìƒˆ ë°ì´í„°ë¥¼ ê¸°ì¡´ 2015-2022ë…„ ë°ì´í„°ì™€ ë³‘í•©"""
    print("\nğŸ”„ ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ë³‘í•© ì¤‘...")
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    if existing_data_path.exists():
        print(f"   ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {existing_data_path}")
        existing_df = pd.read_excel(existing_data_path, engine='openpyxl')
        print(f"   ê¸°ì¡´ ë°ì´í„°: {len(existing_df):,}í–‰")
        
        if 'bsns_year' in existing_df.columns:
            existing_years = sorted(existing_df['bsns_year'].unique())
            print(f"   ê¸°ì¡´ ì—°ë„ ë²”ìœ„: {min(existing_years)} ~ {max(existing_years)}")
    else:
        print(f"   âš ï¸ ê¸°ì¡´ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {existing_data_path}")
        existing_df = pd.DataFrame()
    
    # ìƒˆ ë°ì´í„° ë¡œë“œ
    if new_data_path.exists():
        print(f"   ìƒˆ ë°ì´í„° ë¡œë“œ: {new_data_path}")
        new_df = pd.read_excel(new_data_path, engine='openpyxl')
        print(f"   ìƒˆ ë°ì´í„°: {len(new_df):,}í–‰")
        
        if 'bsns_year' in new_df.columns:
            new_years = sorted(new_df['bsns_year'].unique())
            print(f"   ìƒˆ ì—°ë„ ë²”ìœ„: {min(new_years)} ~ {max(new_years)}")
    else:
        print(f"   âŒ ìƒˆ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {new_data_path}")
        return
    
    # ë°ì´í„° ë³‘í•©
    if not existing_df.empty:
        combined_df = pd.concat([existing_df, new_df], ignore_index=True)
        print(f"   ë³‘í•©ëœ ì „ì²´ ë°ì´í„°: {len(combined_df):,}í–‰")
    else:
        combined_df = new_df
        print(f"   ê¸°ì¡´ ë°ì´í„°ê°€ ì—†ì–´ì„œ ìƒˆ ë°ì´í„°ë§Œ ì €ì¥: {len(combined_df):,}í–‰")
    
    # ìµœì¢… íŒŒì¼ ì €ì¥
    save_to_excel(combined_df, final_output_path)
    print(f"âœ… ìµœì¢… ë³‘í•© ì™„ë£Œ!")
    print(f"   ì €ì¥ ìœ„ì¹˜: {final_output_path}")
    
    # ì—°ë„ë³„ ë¶„í¬ í™•ì¸
    if 'bsns_year' in combined_df.columns:
        year_counts = combined_df['bsns_year'].value_counts().sort_index()
        print(f"   ìµœì¢… ì—°ë„ë³„ ë¶„í¬:")
        for year, count in year_counts.items():
            print(f"     {year}ë…„: {count:,}ê°œ ê¸°ì—…")


async def main():
    parser = argparse.ArgumentParser(description='DART 2023-2024ë…„ ì¬ë¬´ì œí‘œ ë‹¤ìš´ë¡œë“œ')
    parser.add_argument('--team', type=int, help='íŒ€ ë²ˆí˜¸ (ì˜ˆ: 1, 2, 3...)')
    parser.add_argument('--merge-only', action='store_true', help='2023-2024ë…„ íŒ€ë³„ íŒŒì¼ ë³‘í•©ë§Œ ìˆ˜í–‰')
    parser.add_argument('--merge-all', action='store_true', help='ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ì „ì²´ ë³‘í•©')
    parser.add_argument('--list-teams', action='store_true', help='íŒ€ ë¶„í•  ì •ë³´ í‘œì‹œ')
    parser.add_argument('--workers', type=int, default=10, help='ë™ì‹œ ì‘ì—… ìˆ˜ (ê¸°ë³¸ê°’: 10)')
    
    args = parser.parse_args()
    
    # API í‚¤ í™•ì¸
    api_key = os.getenv("DART_API_KEY")
    if not api_key:
        raise EnvironmentError("DART_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(__file__).resolve().parent.parent / "data" / "team_downloads"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ë°ì´í„° ì „ì²´ ë³‘í•©
    if args.merge_all:
        new_data_path = output_dir.parent / "dart_statements_2023_2024_merged.xlsx"
        existing_data_path = output_dir.parent / "dart_statements_merged.xlsx"  # ê¸°ì¡´ 2015-2022 ë°ì´í„°
        final_output_path = output_dir.parent / "dart_statements_2015_2024_merged.xlsx"
        
        merge_with_existing_data(new_data_path, existing_data_path, final_output_path)
        return
    
    # 2023-2024ë…„ íŒ€ë³„ íŒŒì¼ ë³‘í•©ë§Œ ìˆ˜í–‰
    if args.merge_only:
        team_files = list(output_dir.glob("dart_statements_2023_2024_team_*.xlsx"))
        if team_files:
            final_output = output_dir.parent / "dart_statements_2023_2024_merged.xlsx"
            merge_team_files_2023_2024(team_files, final_output)
        else:
            print("ë³‘í•©í•  2023-2024ë…„ íŒ€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ê¸°ì—… ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
    print("ğŸ“‹ ê¸°ì—… ì½”ë“œ ëª©ë¡ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    corp_df = await fetch_corp_codes(api_key)
    target_df = filter_kospi_kosdaq_non_financial(corp_df)
    
    # ì „ì²´ ê¸°ì—… ì½”ë“œ (ê¸°ì¡´ê³¼ ë™ì¼í•œ ì¡°ê±´)
    all_corp_codes = target_df["corp_code"].unique().tolist()
    print(f"   ì „ì²´ ëŒ€ìƒ ê¸°ì—…: {len(all_corp_codes):,}ê°œ")
    
    # íŒ€ë³„ë¡œ ë¶„í•  (ê¸°ì¡´ê³¼ ë™ì¼)
    team_chunks = split_corps_for_teams(all_corp_codes, chunk_size=100)
    
    # íŒ€ ì •ë³´ í‘œì‹œ
    if args.list_teams:
        print("\nğŸ“Š íŒ€ë³„ í• ë‹¹ ì •ë³´ (2023-2024ë…„ ë‹¤ìš´ë¡œë“œìš©):")
        for team_num, corps in team_chunks:
            print(f"   íŒ€ {team_num}: {len(corps)}ê°œ ê¸°ì—… (ì¸ë±ìŠ¤ {(team_num-1)*100} ~ {team_num*100-1})")
        print(f"\nğŸ’¡ ì‚¬ìš©ë²•:")
        print(f"   1. íŒ€ë³„ ë‹¤ìš´ë¡œë“œ: python {Path(__file__).name} --team [íŒ€ë²ˆí˜¸]")
        print(f"   2. 2023-2024 íŒŒì¼ ë³‘í•©: python {Path(__file__).name} --merge-only")
        print(f"   3. ì „ì²´ ë°ì´í„° ë³‘í•©: python {Path(__file__).name} --merge-all")
        return
    
    # íŠ¹ì • íŒ€ ë‹¤ìš´ë¡œë“œ (2023-2024ë…„)
    if args.team:
        team_data = next((chunk for chunk in team_chunks if chunk[0] == args.team), None)
        if team_data:
            team_num, corp_codes = team_data
            years = range(2023, 2025)  # 2023, 2024ë…„
            
            print(f"\nğŸ¯ íŒ€ {team_num} (2023-2024ë…„) ë‹¤ìš´ë¡œë“œ ì„¤ì •:")
            print(f"   - ê¸°ì—… ìˆ˜: {len(corp_codes)}ê°œ")
            print(f"   - ì—°ë„: 2023 ~ 2024")
            print(f"   - ì˜ˆìƒ ìš”ì²­ ìˆ˜: {len(corp_codes) * 2:,}ê°œ")  # 2ë…„ì¹˜
            print(f"   - ë™ì‹œ ì‘ì—… ìˆ˜: {args.workers}")
            
            await download_team_data_2023_2024(
                api_key=api_key,
                team_num=team_num,
                corp_codes=corp_codes,
                years=years,
                output_dir=output_dir,
                workers=args.workers
            )
        else:
            print(f"âŒ íŒ€ {args.team}ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print(f"   ì‚¬ìš© ê°€ëŠ¥í•œ íŒ€: 1 ~ {len(team_chunks)}")
    else:
        print("\nğŸ’¡ 2023-2024ë…„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‚¬ìš©ë²•:")
        print(f"  1. íŒ€ ì •ë³´ í™•ì¸: python {Path(__file__).name} --list-teams")
        print(f"  2. íŒ€ë³„ ë‹¤ìš´ë¡œë“œ: python {Path(__file__).name} --team [íŒ€ë²ˆí˜¸]")
        print(f"  3. 2023-2024 íŒŒì¼ ë³‘í•©: python {Path(__file__).name} --merge-only")
        print(f"  4. ì „ì²´ ë°ì´í„° ë³‘í•©: python {Path(__file__).name} --merge-all")
        print("\nì˜µì…˜:")
        print("  --workers N: ë™ì‹œ ì‘ì—… ìˆ˜ ì„¤ì • (ê¸°ë³¸ê°’: 10)")
        print("\nğŸ“ ì°¸ê³ :")
        print("  - ê¸°ì¡´ 2015-2022ë…„ ë°ì´í„°ì™€ ë™ì¼í•œ í•„í„°ë§ ì¡°ê±´ ì ìš©")
        print("  - KOSPI/KOSDAQ ë¹„ê¸ˆìœµì—…ì¢… ê¸°ì—… ëŒ€ìƒ")
        print("  - ì—°ê²°ì¬ë¬´ì œí‘œ(CFS) ìš°ì„ , ì—†ìœ¼ë©´ ê°œë³„ì¬ë¬´ì œí‘œ(OFS) ìˆ˜ì§‘")


if __name__ == "__main__":
    asyncio.run(main())
