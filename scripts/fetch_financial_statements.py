import os
import sys
import asyncio
import aiohttp
import pandas as pd
from pathlib import Path
from tqdm.asyncio import tqdm
import argparse
import pickle

SRC_PATH = Path(__file__).resolve().parent.parent / "src"
sys.path.append(str(SRC_PATH))

from dart_bulk_downloader import (
    fetch_corp_codes,
    filter_kospi_kosdaq_non_financial,
    fetch_single_statement,
    RateLimiter,
)

PROGRESS_PATH = Path(__file__).resolve().parent.parent / "data" / "raw" / "financial_statements_progress.csv"
CACHE_FILE = Path(__file__).resolve().parent.parent / "data" / "corp_codes_cache.pkl"
BATCH_SIZE = 100

async def get_corp_codes_with_cache(api_key: str, force_refresh: bool = False) -> pd.DataFrame:
    CACHE_FILE.parent.mkdir(parents=True, exist_ok=True)
    if CACHE_FILE.exists() and not force_refresh:
        try:
            print("ğŸ’¾ ìºì‹œëœ ê¸°ì—… ì½”ë“œ ë¡œë“œ ì¤‘...")
            with open(CACHE_FILE, "rb") as f:
                corp_df = pickle.load(f)
            print(f"âœ… ìºì‹œì—ì„œ {len(corp_df)}ê°œ ê¸°ì—… ë¡œë“œ")
            return corp_df
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}, APIì—ì„œ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤")

    corp_df = await fetch_corp_codes(api_key)
    try:
        with open(CACHE_FILE, "wb") as f:
            pickle.dump(corp_df, f)
        print(f"ğŸ’¾ ê¸°ì—… ì½”ë“œ ëª©ë¡ì„ ìºì‹œì— ì €ì¥: {CACHE_FILE}")
    except Exception as e:
        print(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    return corp_df

def save_csv(df: pd.DataFrame, filename: str) -> None:
    out_dir = Path(__file__).resolve().parent.parent / "data" / "raw"
    out_dir.mkdir(parents=True, exist_ok=True)
    path = out_dir / filename
    df.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"ğŸ“ Saved {len(df):,} rows -> {path}")

async def main(reset: bool = False, use_cache: bool = True):
    api_key = os.getenv("DART_API_KEY")
    if not api_key:
        raise EnvironmentError("DART_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    if reset and PROGRESS_PATH.exists():
        PROGRESS_PATH.unlink()
        print("ğŸ—‘ï¸ ê¸°ì¡´ ì§„í–‰ íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    print("ğŸ“¥ ê¸°ì—… ì½”ë“œ ìˆ˜ì§‘ ì¤‘...")
    corp_df = await get_corp_codes_with_cache(api_key, force_refresh=not use_cache)
    target_df = filter_kospi_kosdaq_non_financial(corp_df)
    corp_codes = target_df["corp_code"].unique().tolist()
    corp_name_map = dict(zip(target_df["corp_code"], target_df["corp_name"]))
    print(f"âœ… ë¹„ê¸ˆìœµ ìƒì¥ê¸°ì—… ìˆ˜: {len(corp_codes)}")

    years = list(range(2015, 2024))
    collected = []
    collected_keys = set()

    if PROGRESS_PATH.exists():
        collected_df = pd.read_csv(PROGRESS_PATH, dtype=str)
        collected_keys = set(zip(collected_df["corp_code"], collected_df["bsns_year"], collected_df["fs_div"]))
        collected = [collected_df]
        print(f"ğŸ“‹ ê¸°ì¡´ ìˆ˜ì§‘ëœ ë°ì´í„°: {len(collected_keys):,}ê°œ ì‘ì—… ì™„ë£Œ")
    else:
        print("ğŸ“‹ ìƒˆë¡œìš´ ìˆ˜ì§‘ ì‹œì‘")

    sem = asyncio.Semaphore(10)
    rate_limiter = RateLimiter(max_calls=500, period=60)
    stop_event = asyncio.Event()

    pending_tasks = []
    total_tasks = len(corp_codes) * len(years) * 2

    for corp_code in corp_codes:
        for year in years:
            for fs_div in ["CFS", "OFS"]:
                key = (corp_code, str(year), fs_div)
                if key not in collected_keys:
                    pending_tasks.append((corp_code, year, fs_div))

    completed_tasks = total_tasks - len(pending_tasks)
    print(f"ğŸ“Š ì „ì²´ ì‘ì—…: {total_tasks:,}ê°œ / ì™„ë£Œ: {completed_tasks:,}ê°œ / ë‚¨ì€ ì‘ì—…: {len(pending_tasks):,}ê°œ")
    if not pending_tasks:
        print("âœ… ëª¨ë“  ì‘ì—…ì´ ì´ë¯¸ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("ğŸš€ ë³‘ë ¬ ìˆ˜ì§‘ ì‹œì‘...")

    async def worker(session, corp_code, year, fs_div):
        if stop_event.is_set():
            return
        async with sem:
            try:
                print(f"ğŸ” ìš”ì²­: {corp_code}, {year}, {fs_div}")
                df = await fetch_single_statement(session, rate_limiter, api_key, corp_code, year)
                print(f"ğŸ“Š ê²°ê³¼: {corp_code} {year} {fs_div} â†’ df.empty={df.empty}, columns={list(df.columns)}")
            except RuntimeError as e:
                error_msg = str(e)
                if "ì¡°íšŒëœ ë°ì´íƒ€ê°€ ì—†ìŠµë‹ˆë‹¤" in error_msg:
                    return
                else:
                    print(f"âŒ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ: {e}")
                    stop_event.set()
                    return

        if stop_event.is_set() or df.empty:
            return

        print(f"âœ… ë°ì´í„° ìˆ˜ì§‘ë¨: {corp_code} {year} {fs_div}, í–‰ ìˆ˜: {len(df)}")
        df["corp_name"] = corp_name_map.get(corp_code, "")
        df["corp_code"] = corp_code
        df["bsns_year"] = year
        df["fs_div"] = fs_div
        collected.append(df)

        header = not PROGRESS_PATH.exists()
        df.to_csv(PROGRESS_PATH, mode="a", header=header, index=False, encoding="utf-8-sig")
        collected_keys.add((corp_code, str(year), fs_div))

    if pending_tasks:
        async with aiohttp.ClientSession() as session:
            for start in range(0, len(pending_tasks), BATCH_SIZE):
                if stop_event.is_set():
                    print("ğŸ“‰ ì¤‘ë‹¨ ì¡°ê±´ ë°œìƒ. ìˆ˜ì§‘ ì¢…ë£Œ.")
                    break

                batch = pending_tasks[start:start + BATCH_SIZE]
                tasks = [asyncio.create_task(worker(session, corp_code, year, fs_div)) for corp_code, year, fs_div in batch]
                progress_desc = f"ì§„í–‰ë¥  ({completed_tasks:,}/{total_tasks:,} ì™„ë£Œ)"

                for f in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=progress_desc):
                    try:
                        await f
                    except asyncio.CancelledError:
                        pass
                    if stop_event.is_set():
                        for t in tasks:
                            t.cancel()
                        await asyncio.gather(*tasks, return_exceptions=True)
                        break

                if stop_event.is_set():
                    break
                completed_tasks += len(batch)

    if not collected:
        print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    final_df = pd.concat(collected, ignore_index=True)
    cfs_bs = final_df[(final_df["fs_div"] == "CFS") & (final_df["sj_div"] == "BS")]
    cfs_is = final_df[(final_df["fs_div"] == "CFS") & (final_df["sj_div"] == "IS")]
    ofs_bs = final_df[(final_df["fs_div"] == "OFS") & (final_df["sj_div"] == "BS")]
    ofs_is = final_df[(final_df["fs_div"] == "OFS") & (final_df["sj_div"] == "IS")]

    save_csv(cfs_bs, "ì—°ê²°ì¬ë¬´ì œí‘œ_ì¬ë¬´ìƒíƒœí‘œ.csv")
    save_csv(cfs_is, "ì—°ê²°ì¬ë¬´ì œí‘œ_ì†ìµê³„ì‚°ì„œ.csv")
    save_csv(ofs_bs, "ì¬ë¬´ì œí‘œ_ì¬ë¬´ìƒíƒœí‘œ.csv")
    save_csv(ofs_is, "ì¬ë¬´ì œí‘œ_ì†ìµê³„ì‚°ì„œ.csv")
    print("âœ… ì „ì²´ ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Download financial statements from DART")
    parser.add_argument("--reset", action="store_true", help="Ignore progress and download from scratch")
    parser.add_argument("--no-cache", action="store_true", help="Do not use cached corp codes")
    args = parser.parse_args()
    asyncio.run(main(reset=args.reset, use_cache=not args.no_cache))
