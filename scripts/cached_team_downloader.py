"""ìºì‹œë¥¼ í™œìš©í•œ ì•ˆì •ì ì¸ DART íŒ€ ë‹¤ìš´ë¡œë”"""

import asyncio
import os
import pickle
from pathlib import Path
from dart_bulk_downloader import fetch_corp_codes, filter_kospi_kosdaq_non_financial, fetch_bulk_statements, save_to_excel
from dotenv import load_dotenv
import logging

# Load API key
load_dotenv(Path(__file__).resolve().parent.parent / ".env")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

async def get_corp_codes_with_cache(api_key: str, force_refresh: bool = False):
    """ìºì‹œë¥¼ ì‚¬ìš©í•œ ê¸°ì—… ì½”ë“œ ê°€ì ¸ì˜¤ê¸°"""
    cache_file = Path(__file__).resolve().parent.parent / "data" / "corp_codes_cache.pkl"
    cache_file.parent.mkdir(parents=True, exist_ok=True)
    
    # ìºì‹œ íŒŒì¼ì´ ìˆê³  ê°•ì œ ìƒˆë¡œê³ ì¹¨ì´ ì•„ë‹Œ ê²½ìš°
    if cache_file.exists() and not force_refresh:
        try:
            logger.info("ğŸ’¾ ìºì‹œëœ ê¸°ì—… ì½”ë“œ ëª©ë¡ ë¡œë“œ ì¤‘...")
            with open(cache_file, 'rb') as f:
                corp_df = pickle.load(f)
            
            logger.info(f"âœ… ìºì‹œì—ì„œ {len(corp_df)}ê°œ ê¸°ì—… ë¡œë“œ ì™„ë£Œ")
            return corp_df
            
        except Exception as e:
            logger.warning(f"âš ï¸ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}, APIì—ì„œ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤")
    
    # APIì—ì„œ ìƒˆë¡œ ë‹¤ìš´ë¡œë“œ
    logger.info("ğŸŒ APIì—ì„œ ê¸°ì—… ì½”ë“œ ëª©ë¡ ë‹¤ìš´ë¡œë“œ ì¤‘...")
    corp_df = await fetch_corp_codes(api_key)
    
    # ìºì‹œì— ì €ì¥
    try:
        with open(cache_file, 'wb') as f:
            pickle.dump(corp_df, f)
        logger.info(f"ğŸ’¾ ê¸°ì—… ì½”ë“œ ëª©ë¡ì„ ìºì‹œì— ì €ì¥: {cache_file}")
    except Exception as e:
        logger.warning(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    return corp_df

async def cached_team_download(team_num: int, skip_validation: bool = True, use_cache: bool = True):
    """ìºì‹œë¥¼ í™œìš©í•œ íŒ€ ë‹¤ìš´ë¡œë“œ"""
    api_key = os.getenv("DART_API_KEY")
    if not api_key:
        logger.error("âŒ DART_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # ìºì‹œë¥¼ ì‚¬ìš©í•œ ê¸°ì—… ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
        corp_df = await get_corp_codes_with_cache(api_key, force_refresh=not use_cache)
        target_df = filter_kospi_kosdaq_non_financial(corp_df)
        
        # ì£¼ì‹ì½”ë“œë¡œ ì •ë ¬
        target_df = target_df.sort_values('stock_code').reset_index(drop=True)
        logger.info(f"ì£¼ì‹ì½”ë“œ ìˆœìœ¼ë¡œ ì •ë ¬ë¨ (ì´ {len(target_df)}ê°œ ê¸°ì—…)")
        
        # íŒ€ ì„¤ì •
        chunk_size = 100
        start_idx = (team_num - 1) * chunk_size
        end_idx = min(start_idx + chunk_size, len(target_df))
        
        if start_idx >= len(target_df):
            max_teams = len(target_df) // chunk_size + (1 if len(target_df) % chunk_size > 0 else 0)
            logger.error(f"âŒ íŒ€ {team_num}ì€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤ (ìµœëŒ€ {max_teams}íŒ€)")
            return False
        
        team_corps = target_df.iloc[start_idx:end_idx]
        corp_codes = team_corps['corp_code'].tolist()
        years = range(2015, 2023)
        
        logger.info(f"íŒ€ {team_num} ë‹¤ìš´ë¡œë“œ ì„¤ì •:")
        logger.info(f"- ê¸°ì—… ìˆ˜: {len(corp_codes)}ê°œ")
        logger.info(f"- ì—°ë„: 2015 ~ 2022")
        logger.info(f"- ì˜ˆìƒ ìš”ì²­ ìˆ˜: {len(corp_codes) * len(years)}ê°œ")
        
        # ìƒ˜í”Œ ê¸°ì—…ëª… ì¶œë ¥
        sample_names = team_corps['corp_name'].head(3).tolist()
        logger.info(f"- ìƒ˜í”Œ ê¸°ì—…: {', '.join(sample_names)}")
        
        # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰ (ì†ë„ ì œí•œ ê°•í™”)
        statements = await fetch_bulk_statements(
            api_key, 
            corp_codes, 
            years, 
            workers=3,  # ì›Œì»¤ ìˆ˜ ëŒ€í­ ê°ì†Œ (10â†’3)
            include_corp_names=True
        )
        
        if not statements.empty:
            # íŒŒì¼ ì €ì¥
            output_dir = Path(__file__).resolve().parent.parent / "data" / "team_downloads"
            output_dir.mkdir(parents=True, exist_ok=True)
            output_path = output_dir / f"dart_statements_team_{team_num:02d}.xlsx"
            
            save_to_excel(statements, output_path)
            
            logger.info(f"âœ… íŒ€ {team_num} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ìˆ˜ì§‘ëœ ë°ì´í„°: {len(statements):,}í–‰")
            logger.info(f"ğŸ“ˆ ê¸°ì—…ë‹¹ í‰ê· : {len(statements)//len(corp_codes):.0f}í–‰")
            logger.info(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_path}")
            return True
        else:
            logger.error(f"âŒ íŒ€ {team_num}: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        logger.error(f"âŒ íŒ€ {team_num} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def clear_cache():
    """ìºì‹œ íŒŒì¼ ì‚­ì œ"""
    cache_file = Path(__file__).resolve().parent.parent / "data" / "corp_codes_cache.pkl"
    if cache_file.exists():
        cache_file.unlink()
        print(f"ğŸ—‘ï¸ ìºì‹œ íŒŒì¼ ì‚­ì œ: {cache_file}")
    else:
        print("â„¹ï¸ ìºì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

async def main():
    import argparse
    parser = argparse.ArgumentParser(description="ìºì‹œë¥¼ í™œìš©í•œ DART íŒ€ ë‹¤ìš´ë¡œë”")
    parser.add_argument("--team", type=int, required=True, help="íŒ€ ë²ˆí˜¸")
    parser.add_argument("--skip-validation", action="store_true", help="ìœ íš¨ì„± ê²€ì¦ ìŠ¤í‚µ")
    parser.add_argument("--no-cache", action="store_true", help="ìºì‹œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ")
    parser.add_argument("--clear-cache", action="store_true", help="ìºì‹œ íŒŒì¼ ì‚­ì œ")
    args = parser.parse_args()
    
    if args.clear_cache:
        clear_cache()
        return
    
    logger.info(f"ğŸš€ íŒ€ {args.team} ìºì‹œ í™œìš© ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    
    success = await cached_team_download(
        args.team, 
        args.skip_validation, 
        use_cache=not args.no_cache
    )
    
    if success:
        logger.info(f"ğŸ‰ íŒ€ {args.team} ë‹¤ìš´ë¡œë“œ ì„±ê³µ!")
    else:
        logger.error(f"ğŸ’¥ íŒ€ {args.team} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨!")
        exit(1)

if __name__ == "__main__":
    asyncio.run(main())
